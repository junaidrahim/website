---
title: "Give a Fuck About Words"
date: "2025-04-09T15:24:21+05:30"
summary: "After LLMs, there is too much alpha in being articulate."
description: "After LLMs, there is too much alpha in being articulate."
toc: true
readTime: true
math: true
draft: false
---

In this era of AI and LLMs, being articulate isn't just nice to have, it's becoming the hidden superpower that separates the successful from the merely competent.

Let me explain why.

## Misunderstood Hierarchy of Software Creation

Anyone who doesn't write software thinks the hardest part is coding—the mysterious symbols and syntax that make machines do things. It's easy to be impressed by it if you're not familiar with the process.

But people who write software know that typing code is usually the easiest part. The real challenges lie in:

- Deciding what to build.
- Designing the architecture.
- Choosing the right components.
- Understanding and partitioning the problem creatively and correctly.

If you want to solve these challenges with LLMs, you need to be able to express and articulate complex problems very clearly.

It takes a lot more than just doing `Cmd+K => "Fix this"` on cursor to actually improve your effectiveness as an engineer using AI.

## The Flamegraph of Problems in Companies

Problems in companies look remarkably like flame graphs in performance profiling.

![](https://www.brendangregg.com/FlameGraphs/cpu-bash-flamegraph.svg)

At the bottom, the widest part of the graph, sits the company's mission.

As you move up, you get increasingly specific problems and tasks, each stacking on the foundation below it. Your typical individual contributor works near the top of this graph, solving specific, well-defined problems.

But here's the thing: the deeper down you can operate in this hierarchy, the more multiplicative your impact becomes. Solving a problem one level deeper doesn't just fix one issue—it potentially resolves dozens that stem from it.

And this is where articulation becomes critical.

LLMs like Claude, tools like Cursor—they're exceptionally good at solving specific, well-defined problems. Want to optimize a function? Fix a bug? Generate boilerplate? They've got you.

But to tackle those deeper, wider problems, you need to provide significantly more context. You need to articulate:

- The big-picture goal
- The system constraints
- The historical context
- The interlocking dependencies
- The people bottlenecks

This requires sitting down and thoughtfully writing out or dictating what you're trying to do, or engaging in extended dialogue with these systems to help them understand the broader context.

The more articulate you are, the more you can explain a problem structurally, the deeper in the flame graph you can effectively operate with these tools.

## The LLM Inflection Point

We've now entered an era where LLMs can generate very competent code from a well-articulated prompts. This creates a fascinating inversion:

- **Previously**: Technical skills and experience were the bottleneck.
- **Now**: Articulation skills are the bottleneck

The better you can express your intentions, clarify your needs, and communicate constraints, the better results you'll get from these systems.

And not just marginally better, but _exponentially_ better.

This matches perfectly with the flame graph model. As you move down the graph to more foundational problems, the complexity increases, and simple prompts no longer suffice.

Your ability to articulate complex systems becomes the limiting factor in your collaboration with these AI tools.

## The New Primitives

In traditional programming, primitives are the basic data types and operations a language provides.

In LLM-augmented development, your primitives also include:

- Clarity of expression
- Conceptual decomposition
- Question formulation
- Contextual awareness

These are all fundamentally language skills, not coding skills.

## This is Beyond Tech

This isn't just about software development.

As these tools spread into marketing, design, content creation, business analysis, and virtually every knowledge field, the ability to articulate thoughts clearly becomes the universal competitive advantage.

Someone who can express "I need X that does Y while considering A,B,C and D and also something.json." will consistently outperform the person who can only manage "Make something like X."

In an era where anyone can generate mediocre content at scale, precision of thought and expression becomes the differentiator.

The future belongs to those who can say exactly what they mean.

## What to do about it?

There's too much alpha in being articulate. Above all, **give a fuck about words**.

They're not just how we communicate ideas—increasingly, **they're how we will them into existence**.
